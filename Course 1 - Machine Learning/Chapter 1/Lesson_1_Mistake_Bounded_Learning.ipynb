{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mistake Bounded Learning"
      ],
      "metadata": {
        "id": "SgSxyv9lcqTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistake Bounded Learning is a concept in machine learning that emphasizes the idea of limiting the number of mistakes or errors made by a model during its learning process. This approach is particularly relevant in scenarios where minimizing errors or failures is critical, such as in safety-critical applications or when dealing with costly mistakes.\n",
        "\n",
        "In traditional machine learning, the primary focus is often on optimizing performance metrics like accuracy, precision, or recall. However, in certain real-world applications, simply maximizing accuracy may not be sufficient. For instance, in autonomous driving, a model should not only aim to make correct predictions but also minimize critical mistakes like misidentifying pedestrians or other vehicles.\n",
        "\n",
        "Mistake Bounded Learning introduces a different perspective by explicitly incorporating the notion of acceptable error rates or mistake thresholds into the learning process. The goal is to train models that not only perform well in terms of conventional metrics but also adhere to predefined limits on mistakes or errors."
      ],
      "metadata": {
        "id": "fK3wl7-tcr9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Key Concepts of Mistake Bounded Learning:"
      ],
      "metadata": {
        "id": "9E7VDtk1ctWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Defining Acceptable Mistake Levels: Before training a model, specific thresholds for mistakes or errors need to be defined based on the application requirements. These thresholds could vary depending on the severity and impact of mistakes in different contexts.\n",
        "\n",
        "2. Optimizing for Mistake Reduction: The learning algorithm is adapted to focus not just on optimizing performance metrics but also on actively minimizing mistakes during training. This may involve specialized loss functions or training strategies that penalize certain types of mistakes more heavily.\n",
        "\n",
        "3. Trade-offs between Accuracy and Mistakes: Mistake Bounded Learning often involves exploring trade-offs between traditional performance metrics (like accuracy) and mistake reduction. It recognizes that in some cases, sacrificing a small amount of accuracy might be necessary to ensure a model stays within acceptable mistake limits.\n",
        "\n",
        "4. Dynamic Mistake Thresholds: In dynamic environments, mistake thresholds might need to be adjusted over time based on evolving conditions or feedback from the model's performance in real-world scenarios."
      ],
      "metadata": {
        "id": "alk55zQPcuwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv5WHgbZcWWX",
        "outputId": "cf54a38e-aeb3-41ec-b502-323d29abf8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 2s 15ms/step - loss: 2.0943 - accuracy: 0.5500\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 1.0119 - accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6989 - accuracy: 0.9625\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.5294 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.4833 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.4405 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.4005 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3622 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3262 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2921 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0a0c2db430>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Convert to binary classification task\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=100).batch(32)\n",
        "\n",
        "# Define a simple neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(4,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Custom mistake bounded loss function\n",
        "def mistake_bounded_loss(y_true, y_pred):\n",
        "    mistake_threshold = 0.1\n",
        "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
        "    error_count = tf.reduce_sum(tf.abs(y_true - tf.round(y_pred)))\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + mistake_threshold * error_count\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=mistake_bounded_loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The 5 most important methods for Mistake Bounded Learning"
      ],
      "metadata": {
        "id": "3vae_8N3drIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Customized Loss Functions:\n",
        "One of the fundamental methods for Mistake Bounded Learning is the design and implementation of customized loss functions that explicitly penalize specific types of mistakes. These loss functions typically combine traditional components like cross-entropy with additional terms that focus on minimizing targeted errors (e.g., false positives, false negatives). By tailoring the loss function to the desired mistake bounds, the model learns to prioritize reducing critical errors during training.\n"
      ],
      "metadata": {
        "id": "44sw5rPMd7z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def mistake_bounded_loss(y_true, y_pred):\n",
        "    mistake_threshold = 0.1  # Define your mistake threshold here\n",
        "    y_true_float = tf.cast(y_true, tf.float32)\n",
        "    error_count = tf.reduce_sum(tf.abs(y_true_float - tf.round(y_pred)))\n",
        "    binary_crossentropy = tf.keras.losses.binary_crossentropy(y_true_float, y_pred)\n",
        "    return binary_crossentropy + mistake_threshold * error_count\n",
        "\n",
        "# Compile the model using the customized loss function\n",
        "model.compile(optimizer='adam', loss=mistake_bounded_loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23bBNTued7at",
        "outputId": "ac3387be-160c-4809-83b2-49973d9b5aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 3s 7ms/step - loss: 0.2597 - accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.2275 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1987 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1515 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1316 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1142 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0986 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.0735 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0a0c30a2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Thresholding and Decision Policies:"
      ],
      "metadata": {
        "id": "x8FSj2IGelAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistake Bounded Learning often involves setting decision thresholds or policies that control model predictions based on the confidence level or probability scores. By adjusting decision thresholds, models can trade-off between accuracy and mistake reduction. For example, lowering the decision threshold for positive predictions can reduce false negatives (missed detections) at the expense of potentially increasing false positives (false alarms)."
      ],
      "metadata": {
        "id": "mYNj81v2erCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_threshold(model, X, threshold=0.5):\n",
        "    y_pred = model.predict(X)\n",
        "    return (y_pred >= threshold).astype(int)\n",
        "\n",
        "# Example: Predict using a lower threshold to reduce false negatives\n",
        "y_pred_adjusted = predict_with_threshold(model, X_test, threshold=0.3)\n",
        "y_pred_adjusted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11JMmtKUee7S",
        "outputId": "0108817f-746c-470f-d129-73fb20301a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 123ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Active Learning and Data Selection:"
      ],
      "metadata": {
        "id": "s06ZGI91fE3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active learning strategies play a crucial role in Mistake Bounded Learning by focusing model training on informative or challenging examples that are more likely to lead to critical mistakes. Techniques like uncertainty sampling or diversity-based sampling can be employed to prioritize labeling and training on instances where the model is most likely to make mistakes, thereby improving mistake-aware performance."
      ],
      "metadata": {
        "id": "OF72L0-bfF1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into train and pool sets\n",
        "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Implement active learning by selecting challenging examples\n",
        "# Use uncertainty sampling or diversity-based sampling\n",
        "# Train your model iteratively on the selected examples\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=32)  # Example training step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vB3uuRVfKJu",
        "outputId": "aca54d8a-178a-4e0c-f9d8-ec265411c2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 839ms/step - loss: -1.6550 - accuracy: 0.3667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0a0c2ae4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Regularization Techniques:\n"
      ],
      "metadata": {
        "id": "hdrRMQr2fmJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization methods such as penalty terms or constraints can be used to enforce mistake bounds during training. For instance, incorporating constraints that limit the allowable error rates or incorporating penalty terms that discourage extreme predictions can guide the model towards making more conservative and mistake-aware decisions."
      ],
      "metadata": {
        "id": "yrESdH8IfqJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Convert to binary classification task\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=100).batch(32)\n",
        "\n",
        "# Define a simple neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(4,)),\n",
        "    Dense(32, activation='relu'),\n",
        "])\n",
        "\n",
        "# Custom mistake bounded loss function\n",
        "def mistake_bounded_loss(y_true, y_pred):\n",
        "    mistake_threshold = 0.1\n",
        "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
        "    error_count = tf.reduce_sum(tf.abs(y_true - tf.round(y_pred)))\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + mistake_threshold * error_count\n",
        "\n",
        "# Add regularization to the model\n",
        "model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=mistake_bounded_loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWa-kFo7fn9g",
        "outputId": "4cc0c4c0-325b-4db3-b31c-e8499da783e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 3s 11ms/step - loss: 14.7300 - accuracy: 0.3750\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 14.0925 - accuracy: 0.4625\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 13.1109 - accuracy: 0.7000\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 11.9774 - accuracy: 0.9750\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 11.6320 - accuracy: 0.9875\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 11.2945 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 11.0077 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 10.7301 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 10.4578 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 10.1923 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a09fead61a0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Dynamic Threshold Adaptation:"
      ],
      "metadata": {
        "id": "n8c8w9PIghJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistake Bounded Learning can benefit from adaptive strategies that dynamically adjust mistake thresholds based on evolving conditions or performance feedback. Techniques like reinforcement learning or online learning algorithms can be leveraged to continuously optimize mistake bounds in response to changing data distributions or operational requirements."
      ],
      "metadata": {
        "id": "3vhyXpHUg-fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "def calculate_metric(y_true, y_pred, metric):\n",
        "    if metric == 'recall':\n",
        "        return recall_score(y_true, y_pred)\n",
        "    # Add other metrics as needed (e.g., precision, F1-score, etc.)\n",
        "\n",
        "def adapt_threshold(model, X, y, target_metric='recall', target_value=0.9):\n",
        "    threshold = 0.5\n",
        "    while True:\n",
        "        y_pred_adjusted = (model.predict(X) >= threshold).astype(int)\n",
        "        current_metric = calculate_metric(y, y_pred_adjusted, target_metric)\n",
        "\n",
        "        if current_metric >= target_value:\n",
        "            break\n",
        "\n",
        "        threshold -= 0.01\n",
        "\n",
        "    return threshold\n",
        "\n",
        "# Example: Adapt mistake threshold to achieve target recall\n",
        "optimal_threshold = adapt_threshold(model, X_test_scaled, y_test, target_metric='recall', target_value=0.9)\n",
        "print(\"Optimal Threshold:\", optimal_threshold)\n",
        "3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbWFVEM0gg_N",
        "outputId": "3f0ba15a-6ece-4100-be84-3c6e9c5a0948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 155ms/step\n",
            "Optimal Threshold: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimization Techniques for Mistake Bounded Learning"
      ],
      "metadata": {
        "id": "b3WsK2dvh_97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Class-Weighted Loss Function"
      ],
      "metadata": {
        "id": "VKfQkAxLiFWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a class-weighted loss function can help prioritize the reduction of mistakes for specific classes (e.g., reducing false positives or false negatives). This technique assigns higher weights to classes that are more critical to minimize mistakes on."
      ],
      "metadata": {
        "id": "UdULzJnSiToI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Calculate class weights based on training data\n",
        "class_counts = np.bincount(y_train)\n",
        "total_samples = np.sum(class_counts)\n",
        "class_weights = {i: total_samples / (len(class_counts) * class_counts[i]) for i in range(len(class_counts))}\n",
        "\n",
        "# Define a class-weighted loss function\n",
        "def weighted_loss(class_weights):\n",
        "    def loss_function(y_true, y_pred):\n",
        "        weights = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "        weighted_loss = loss * weights\n",
        "        return tf.reduce_mean(weighted_loss)\n",
        "    return loss_function\n",
        "\n",
        "# Define and compile the model with the weighted loss function\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=weighted_loss(list(class_weights.values())), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE9xMK_OiGru",
        "outputId": "ba18c2d4-d8e5-467e-c595-849319cb6c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 3ms/step - loss: 0.2542 - accuracy: 0.9273 - val_loss: 0.1489 - val_accuracy: 0.9557\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1113 - accuracy: 0.9669 - val_loss: 0.1019 - val_accuracy: 0.9705\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0769 - accuracy: 0.9768 - val_loss: 0.0885 - val_accuracy: 0.9723\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0578 - accuracy: 0.9819 - val_loss: 0.0796 - val_accuracy: 0.9761\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0450 - accuracy: 0.9863 - val_loss: 0.0703 - val_accuracy: 0.9783\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0358 - accuracy: 0.9889 - val_loss: 0.0740 - val_accuracy: 0.9767\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0287 - accuracy: 0.9912 - val_loss: 0.0751 - val_accuracy: 0.9778\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0229 - accuracy: 0.9932 - val_loss: 0.0694 - val_accuracy: 0.9801\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.0728 - val_accuracy: 0.9781\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.0843 - val_accuracy: 0.9781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a09fb3417e0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Focal Loss"
      ],
      "metadata": {
        "id": "Watk4JWkjpUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focal Loss is designed to address class imbalance and focuses training on hard examples by down-weighting well-classified examples. This can be effective for reducing mistakes on minority classes or challenging samples."
      ],
      "metadata": {
        "id": "j3t-pMvGjwWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "        # Convert y_true to float32 for compatibility\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "        # Compute focal loss components\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        focal_loss = alpha * tf.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Reduce mean to get final loss value\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Define and compile the model with the focal loss function\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vy1zfZ1jqvT",
        "outputId": "5e1a2b05-ce09-477d-f938-ddac6d1d9de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0392 - accuracy: 0.0700 - val_loss: 1.1022e-05 - val_accuracy: 0.0704\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 3.1852e-06 - accuracy: 0.0675 - val_loss: 1.2067e-06 - val_accuracy: 0.0635\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 4.4868e-07 - accuracy: 0.0644 - val_loss: 2.1764e-07 - val_accuracy: 0.0616\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 9.1944e-08 - accuracy: 0.0623 - val_loss: 8.2457e-08 - val_accuracy: 0.0612\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 2.6456e-08 - accuracy: 0.0612 - val_loss: 3.0892e-08 - val_accuracy: 0.0599\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 5.2543e-09 - accuracy: 0.0587 - val_loss: 1.4287e-08 - val_accuracy: 0.0573\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 6.5724e-10 - accuracy: 0.0580 - val_loss: 1.0654e-08 - val_accuracy: 0.0557\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 1.0222e-10 - accuracy: 0.0563 - val_loss: 8.6413e-09 - val_accuracy: 0.0555\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 3.3209e-12 - accuracy: 0.0565 - val_loss: 8.3753e-09 - val_accuracy: 0.0558\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 6.4165e-13 - accuracy: 0.0569 - val_loss: 8.1395e-09 - val_accuracy: 0.0556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a09fb2eec50>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Early Stopping"
      ],
      "metadata": {
        "id": "Ark9Mdf5ma9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Calculate class weights based on training data\n",
        "class_counts = np.bincount(y_train)\n",
        "total_samples = np.sum(class_counts)\n",
        "class_weights = {i: total_samples / (len(class_counts) * class_counts[i]) for i in range(len(class_counts))}\n",
        "\n",
        "# Define a class-weighted loss function\n",
        "def weighted_loss(class_weights):\n",
        "    def loss_function(y_true, y_pred):\n",
        "        weights = tf.gather(class_weights, tf.cast(y_true, tf.int32))\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "        weighted_loss = loss * weights\n",
        "        return tf.reduce_mean(weighted_loss)\n",
        "    return loss_function\n",
        "\n",
        "# Define and compile the model with the weighted loss function\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Define and compile the model\n",
        "model.compile(optimizer='adam', loss=weighted_loss(list(class_weights.values())), metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK6RZLHYmb_j",
        "outputId": "3538c6dd-1261-45da-bae0-3dba1668cce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2620 - accuracy: 0.9234 - val_loss: 0.1335 - val_accuracy: 0.9622\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1139 - accuracy: 0.9661 - val_loss: 0.1016 - val_accuracy: 0.9683\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0785 - accuracy: 0.9766 - val_loss: 0.0948 - val_accuracy: 0.9703\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0584 - accuracy: 0.9818 - val_loss: 0.0835 - val_accuracy: 0.9743\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0456 - accuracy: 0.9853 - val_loss: 0.0769 - val_accuracy: 0.9763\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0351 - accuracy: 0.9889 - val_loss: 0.0786 - val_accuracy: 0.9771\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.0776 - val_accuracy: 0.9762\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.0795 - val_accuracy: 0.9784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a09f778a380>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Ensemble Method (VotingClassifier)"
      ],
      "metadata": {
        "id": "ayQXUkA5m-hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load MNIST dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize base classifiers\n",
        "log_clf = LogisticRegression(max_iter=1000)\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'  # Use soft voting to predict class probabilities\n",
        ")\n",
        "\n",
        "# Train the ensemble classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate ensemble classifier\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Classifier Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45-TT0PX1ed_",
        "outputId": "7efb3608-d361-43d5-d391-3aae954d1da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}