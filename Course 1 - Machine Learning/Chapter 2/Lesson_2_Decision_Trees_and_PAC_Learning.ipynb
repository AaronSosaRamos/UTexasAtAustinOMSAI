{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Trees\n"
      ],
      "metadata": {
        "id": "85uqq6MqRbZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition and Structure\n",
        "\n",
        "1. Definition: A Decision Tree is a supervised learning algorithm that partitions the feature space into a tree-like structure based on feature values, with the aim of predicting a target variable.\n",
        "2. Structure: The tree consists of nodes where each node represents a feature test, and branches represent the outcomes of the test leading to subsequent nodes or leaf nodes (terminal nodes) representing the final class prediction."
      ],
      "metadata": {
        "id": "xD9boRSpRcfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Criteria\n",
        "\n",
        "* Information Gain: Measures the reduction in entropy or impurity after a dataset is split.\n",
        "* Gini Impurity: Measures the probability of incorrectly classifying a randomly chosen element if it were randomly classified."
      ],
      "metadata": {
        "id": "DaDEOyxqSEnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Prediction\n",
        "\n",
        "* Training: Recursive partitioning of the data based on feature values to maximize information gain or minimize impurity.\n",
        "* Prediction: Traversing the tree from the root to a leaf node based on feature values to predict the target class."
      ],
      "metadata": {
        "id": "aDfEjlT2SJVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advantages and Disadvantages\n",
        "\n",
        "* Advantages: Interpretable, can handle both numerical and categorical data, and relatively efficient for training and inference.\n",
        "* Disadvantages: Prone to overfitting, sensitive to small variations in the training data."
      ],
      "metadata": {
        "id": "mTIR7feXSPsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applications\n",
        "\n",
        "* Classification: Used extensively for classification tasks.\n",
        "* Regression: Decision Trees can also be used for regression tasks."
      ],
      "metadata": {
        "id": "3Lfuzd8TSZq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfKGEm0bQjP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc14d6ed-6dc9-4ae5-d645-054005dcb98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.8711428571428571\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PAC Learning"
      ],
      "metadata": {
        "id": "tzhNpFB3TR0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably Approximately Correct (PAC) learning is a theoretical framework in machine learning that provides guarantees on the generalization performance of learning algorithms."
      ],
      "metadata": {
        "id": "uXangNguTSiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Sample Complexity: Understanding how many training examples are needed to ensure a learner's generalization error is within a certain bound.\n",
        "\n",
        "b. Growth Function: Analysis of the growth function representing the number of distinct labeled datasets that can be realized by a hypothesis class.\n",
        "\n",
        "c. VC Dimension: The Vapnik-Chervonenkis (VC) dimension measures the capacity of a hypothesis class to shatter points in a dataset, providing insights into the learnability of the class.\n",
        "\n",
        "d. Generalization Bounds: Theoretical guarantees on the expected difference between training error and true error based on sample size and hypothesis class complexity."
      ],
      "metadata": {
        "id": "7dQxoxNMTT-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "from math import log, ceil\n",
        "\n",
        "def vc_dimension_decision_tree(max_depth, num_features):\n",
        "    # VC Dimension estimation for Decision Trees\n",
        "    if max_depth is None:\n",
        "        # Infinite VC Dimension for unrestricted depth\n",
        "        return float('inf')\n",
        "    else:\n",
        "        # Calculate VC Dimension based on the maximum depth and number of features\n",
        "        return 2 * max_depth * num_features + 1\n",
        "\n",
        "def sample_complexity_decision_tree(max_depth, num_features, target_error, confidence):\n",
        "    # Estimate VC Dimension of the Decision Tree hypothesis class\n",
        "    vc_dimension = vc_dimension_decision_tree(max_depth, num_features)\n",
        "\n",
        "    # Calculate sample complexity using VC Dimension\n",
        "    sample_size = ceil((vc_dimension * log(1 / confidence)) / target_error**2)\n",
        "\n",
        "    return sample_size\n",
        "\n",
        "# Load the MNIST dataset\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Determine the number of features in the dataset\n",
        "num_features = X.shape[1]\n",
        "\n",
        "# Initialize a DecisionTree Classifier (you can adjust max_depth as needed)\n",
        "max_depth = 10  # Example: Set the maximum depth of the DecisionTree\n",
        "dt_classifier = DecisionTreeClassifier(max_depth=max_depth)\n",
        "\n",
        "# Estimate sample complexity for the DecisionTree Classifier\n",
        "target_error = 0.05  # Example: Desired error rate (5%)\n",
        "confidence = 0.95  # Example: Desired confidence level (95%)\n",
        "sample_size = sample_complexity_decision_tree(max_depth, num_features, target_error, confidence)\n",
        "\n",
        "print(f\"Estimated sample size needed: {sample_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwQZIs2DT_bN",
        "outputId": "20ccc95a-81a5-4f49-8fef-03864352304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated sample size needed: 321733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Tuning for Decision Trees (With Cross-Validation)"
      ],
      "metadata": {
        "id": "WJ3zniGjUD1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and retrain the model\n",
        "best_params = grid_search.best_params_\n",
        "best_dt_classifier = DecisionTreeClassifier(**best_params)\n",
        "best_dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_tuned = best_dt_classifier.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"Tuned Decision Tree Accuracy: {accuracy_tuned}\")"
      ],
      "metadata": {
        "id": "hrDZ4PNIUZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Split data into features and labels\n",
        "X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Engineering Pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, slice(0, X.shape[1]))\n",
        "    ])\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Handling Imbalanced Data - Not implemented in this example\n",
        "\n",
        "# Ensemble Learning with Random Forest\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('classifier', RandomForestClassifier())])\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3, verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the final model with best parameters\n",
        "best_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                             ('classifier', RandomForestClassifier(**best_params))])\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "etEcRSX3Ur7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Methods (Random Forest)"
      ],
      "metadata": {
        "id": "ALlsASX1VZZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx0Gs3AIVYQV",
        "outputId": "b1393e20-9273-4710-abf3-caa3f3123bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9672857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced topics on PAC:\n",
        "1. Online and Active Learning:\n",
        "\n",
        "Explore online learning algorithms that update the model continuously as new data arrives. Active learning strategies focus on selecting the most informative instances for labeling, reducing the labeling cost while maintaining model performance.\n",
        "\n",
        "2. Sample Complexity Bounds:\n",
        "\n",
        "Dive deeper into theoretical analysis of sample complexity bounds for various learning problems. Understand how the sample complexity depends on factors such as the complexity of the hypothesis class, the noise level in the data, and the desired level of confidence.\n",
        "\n",
        "3. Non-IID Data and Federated Learning:\n",
        "\n",
        "Study techniques for learning from non-IID (non-identically distributed) data distributions, common in scenarios like federated learning where data is distributed across multiple devices or locations. Federated learning enables training models on decentralized data while preserving privacy and data security."
      ],
      "metadata": {
        "id": "OpDwLDh_WDzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
        "\n",
        "# Define TensorFlow model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Online learning\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Active learning\n",
        "def uncertainty_sampling(X_pool, model, n_instances=100):\n",
        "    probas = model.predict(X_pool)\n",
        "    entropy = -np.sum(probas * np.log(probas + 1e-8), axis=1)\n",
        "    idx = np.argsort(entropy)[-n_instances:]\n",
        "    return idx\n",
        "\n",
        "X_pool = X_train.copy()\n",
        "y_pool = y_train.copy()\n",
        "labeled_idx = []\n",
        "\n",
        "for _ in range(5):  # Perform 5 active learning iterations\n",
        "    query_idx = uncertainty_sampling(X_pool, model)\n",
        "    labeled_idx.extend(query_idx)\n",
        "    X_labeled = X_pool[query_idx]\n",
        "    y_labeled = y_pool[query_idx]\n",
        "    X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        "    y_pool = np.delete(y_pool, query_idx)\n",
        "    model.fit(X_labeled, y_labeled, epochs=1, batch_size=32, verbose=0)\n",
        "\n",
        "# Federated learning\n",
        "def create_federated_data(X_train, y_train, num_clients=10):\n",
        "    federated_data = defaultdict(list)\n",
        "    client_ids = np.random.choice(np.arange(num_clients), size=len(X_train))\n",
        "    for client_id in range(num_clients):\n",
        "        client_indices = np.where(client_ids == client_id)[0]\n",
        "        federated_data[client_id] = (X_train[client_indices], y_train[client_indices])\n",
        "    return federated_data\n",
        "\n",
        "federated_data = create_federated_data(X_train, y_train)\n",
        "\n",
        "def federated_averaging(model, federated_data, num_rounds=10, batch_size=32):\n",
        "    for _ in range(num_rounds):\n",
        "        for client_id, (X_client, y_client) in federated_data.items():\n",
        "            model.fit(X_client, y_client, epochs=1, batch_size=batch_size, verbose=0)\n",
        "        global_weights = model.get_weights()\n",
        "        for layer in range(len(global_weights)):\n",
        "            for client_id, (X_client, _) in federated_data.items():\n",
        "                client_weights = model.get_weights()\n",
        "                global_weights[layer] += client_weights[layer] / len(federated_data)\n",
        "        model.set_weights(global_weights)\n",
        "\n",
        "federated_averaging(model, federated_data)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEVkA5PcU2zU",
        "outputId": "77761166-f311-4016-c473-f72cebf51049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 18s 9ms/step - loss: 0.2707 - accuracy: 0.9237 - val_loss: 0.1214 - val_accuracy: 0.9647\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1223 - accuracy: 0.9640 - val_loss: 0.1013 - val_accuracy: 0.9705\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0828 - accuracy: 0.9754 - val_loss: 0.0998 - val_accuracy: 0.9695\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0613 - accuracy: 0.9809 - val_loss: 0.0729 - val_accuracy: 0.9787\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0474 - accuracy: 0.9856 - val_loss: 0.0781 - val_accuracy: 0.9785\n",
            "1875/1875 [==============================] - 3s 2ms/step\n",
            "1872/1872 [==============================] - 3s 2ms/step\n",
            "1869/1869 [==============================] - 3s 2ms/step\n",
            "1866/1866 [==============================] - 5s 3ms/step\n",
            "1863/1863 [==============================] - 4s 2ms/step\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 39105.8672 - accuracy: 0.9787\n",
            "Test loss: 39105.8671875, Test accuracy: 0.9786999821662903\n"
          ]
        }
      ]
    }
  ]
}