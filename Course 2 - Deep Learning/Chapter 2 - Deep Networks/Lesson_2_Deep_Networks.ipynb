{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Networks"
      ],
      "metadata": {
        "id": "dVc2EQD6Jwpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. CNN"
      ],
      "metadata": {
        "id": "ngjSAK1ULRx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 2: Load and prepare the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Step 4: Define the neural network architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Step 5: Define loss function and optimizer\n",
        "net = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Step 6: Training loop\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 200}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BvK5fqIJwiL",
        "outputId": "7298452f-6421-4932-9729-d0d3ebbce77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 96430040.04it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 200, Loss: 1.8168666952848433\n",
            "Epoch 1, Batch 400, Loss: 1.475939399600029\n",
            "Epoch 1, Batch 600, Loss: 1.437886941730976\n",
            "Epoch 1, Batch 800, Loss: 1.3338669866323472\n",
            "Epoch 1, Batch 1000, Loss: 1.3135102832317351\n",
            "Epoch 1, Batch 1200, Loss: 1.264631651043892\n",
            "Epoch 1, Batch 1400, Loss: 1.2289302706718446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Batch 200, Loss: 1.1367645198106766\n",
            "Epoch 2, Batch 400, Loss: 1.1263438805937767\n",
            "Epoch 2, Batch 600, Loss: 1.1252578487992286\n",
            "Epoch 2, Batch 800, Loss: 1.10054334461689\n",
            "Epoch 2, Batch 1000, Loss: 1.0812892356514932\n",
            "Epoch 2, Batch 1200, Loss: 1.079770558178425\n",
            "Epoch 2, Batch 1400, Loss: 1.075558524429798\n",
            "Epoch 3, Batch 200, Loss: 0.9857586613297462\n",
            "Epoch 3, Batch 400, Loss: 0.9978375771641731\n",
            "Epoch 3, Batch 600, Loss: 0.9597646543383598\n",
            "Epoch 3, Batch 800, Loss: 0.9696293148398399\n",
            "Epoch 3, Batch 1000, Loss: 0.9665106198191643\n",
            "Epoch 3, Batch 1200, Loss: 0.9465541070699692\n",
            "Epoch 3, Batch 1400, Loss: 0.9593505921959877\n",
            "Epoch 4, Batch 200, Loss: 0.8755633534491062\n",
            "Epoch 4, Batch 400, Loss: 0.8804466012120247\n",
            "Epoch 4, Batch 600, Loss: 0.8861808833479882\n",
            "Epoch 4, Batch 800, Loss: 0.8768923822045326\n",
            "Epoch 4, Batch 1000, Loss: 0.8654893162846565\n",
            "Epoch 4, Batch 1200, Loss: 0.8860401554405689\n",
            "Epoch 4, Batch 1400, Loss: 0.8773477427661419\n",
            "Epoch 5, Batch 200, Loss: 0.7842339450120925\n",
            "Epoch 5, Batch 400, Loss: 0.7861025859415531\n",
            "Epoch 5, Batch 600, Loss: 0.8012751576304435\n",
            "Epoch 5, Batch 800, Loss: 0.8141618606448173\n",
            "Epoch 5, Batch 1000, Loss: 0.8398695629835129\n",
            "Epoch 5, Batch 1200, Loss: 0.814277317672968\n",
            "Epoch 5, Batch 1400, Loss: 0.8057925295829773\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. RNN"
      ],
      "metadata": {
        "id": "2c7bBIo_LTdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eACuHru4JUKp",
        "outputId": "58d4b4d0-5fc5-4971-e5e8-0eed58950085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9948cdc7bc70>:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  return torch.tensor(X).float(), torch.tensor(y).float()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([608])) that is different to the input size (torch.Size([608, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([152])) that is different to the input size (torch.Size([152, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3104.47771875\n",
            "Epoch 2, Loss: 2865.2366171875\n",
            "Epoch 3, Loss: 2666.5357890625\n",
            "Epoch 4, Loss: 2488.11859375\n",
            "Epoch 5, Loss: 2324.5311015625\n",
            "Epoch 6, Loss: 2176.04287890625\n",
            "Epoch 7, Loss: 2040.529984375\n",
            "Epoch 8, Loss: 1916.65119140625\n",
            "Epoch 9, Loss: 1803.44687890625\n",
            "Epoch 10, Loss: 1700.12764453125\n",
            "Epoch 11, Loss: 1606.000578125\n",
            "Epoch 12, Loss: 1520.434734375\n",
            "Epoch 13, Loss: 1442.8423984375\n",
            "Epoch 14, Loss: 1372.66919140625\n",
            "Epoch 15, Loss: 1309.38787890625\n",
            "Epoch 16, Loss: 1252.49518359375\n",
            "Epoch 17, Loss: 1201.5095859375\n",
            "Epoch 18, Loss: 1155.9705859375\n",
            "Epoch 19, Loss: 1115.437892578125\n",
            "Epoch 20, Loss: 1079.4917890625\n",
            "Epoch 21, Loss: 1047.732580078125\n",
            "Epoch 22, Loss: 1019.781291015625\n",
            "Epoch 23, Loss: 995.279630859375\n",
            "Epoch 24, Loss: 973.890154296875\n",
            "Epoch 25, Loss: 955.296517578125\n",
            "Epoch 26, Loss: 939.203447265625\n",
            "Epoch 27, Loss: 925.3365703125\n",
            "Epoch 28, Loss: 913.4423203125\n",
            "Epoch 29, Loss: 903.2875703125\n",
            "Epoch 30, Loss: 894.658998046875\n",
            "Epoch 31, Loss: 887.3627109375\n",
            "Epoch 32, Loss: 881.223423828125\n",
            "Epoch 33, Loss: 876.083416015625\n",
            "Epoch 34, Loss: 871.80192578125\n",
            "Epoch 35, Loss: 868.25389453125\n",
            "Epoch 36, Loss: 865.3289609375\n",
            "Epoch 37, Loss: 862.93038671875\n",
            "Epoch 38, Loss: 860.973873046875\n",
            "Epoch 39, Loss: 859.386451171875\n",
            "Epoch 40, Loss: 858.105404296875\n",
            "Epoch 41, Loss: 857.077115234375\n",
            "Epoch 42, Loss: 856.256103515625\n",
            "Epoch 43, Loss: 855.604064453125\n",
            "Epoch 44, Loss: 855.088900390625\n",
            "Epoch 45, Loss: 854.683966796875\n",
            "Epoch 46, Loss: 854.3672109375\n",
            "Epoch 47, Loss: 854.120576171875\n",
            "Epoch 48, Loss: 853.92933203125\n",
            "Epoch 49, Loss: 853.78160546875\n",
            "Epoch 50, Loss: 853.66783203125\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "def generate_sequence(seq_length=20):\n",
        "    return np.random.randint(0, 100, size=(seq_length))\n",
        "\n",
        "# Create input-output pairs for training\n",
        "def create_dataset(seq_length=20, num_samples=1000):\n",
        "    X, y = [], []\n",
        "    for _ in range(num_samples):\n",
        "        sequence = generate_sequence(seq_length)\n",
        "        X.append(sequence[:-1])  # Input sequence (all but the last element)\n",
        "        y.append(sequence[1:])   # Output sequence (all but the first element)\n",
        "    return torch.tensor(X).float(), torch.tensor(y).float()\n",
        "\n",
        "# Define the Recurrent Neural Network (RNN) model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Forward pass through the RNN layer\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        # Flatten the output for the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "# Create training dataset\n",
        "input_size = 1  # Dimensionality of each item in the sequence\n",
        "hidden_size = 64  # Number of features in the RNN hidden state\n",
        "output_size = 1  # Dimensionality of each predicted item in the sequence\n",
        "seq_length = 20  # Length of input sequence\n",
        "num_samples = 1000  # Number of training samples\n",
        "\n",
        "X_train, y_train = create_dataset(seq_length, num_samples)\n",
        "\n",
        "# Define model, loss function, and optimizer\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        # Prepare batch\n",
        "        inputs = X_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.init_hidden(len(inputs))\n",
        "        outputs, _ = model(inputs.unsqueeze(-1), hidden)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets.view(-1))\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / (num_samples / batch_size)}')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Hyperparameters\n",
        "sequence_length = 28  # Length of sequence (number of rows in an image)\n",
        "input_size = 28  # Number of features in each time step (number of columns in an image)\n",
        "hidden_size = 128  # Number of features in the hidden state\n",
        "num_layers = 2  # Number of RNN layers\n",
        "num_classes = 10  # Number of output classes (digits 0-9)\n",
        "batch_size = 100  # Number of samples in each batch\n",
        "num_epochs = 5  # Number of epochs to train the model\n",
        "learning_rate = 0.001  # Learning rate\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvpgLlS8NJKi",
        "outputId": "00b6a34d-6ae3-4652-f4b7-ee30597ee042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 33657539.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 894346.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9033437.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2190975.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the model\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
      ],
      "metadata": {
        "id": "pbSFuGIUNNOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KojrfyBoNSO6",
        "outputId": "89944768-1558-4279-aa28-fca77e48fcd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 1.2772\n",
            "Epoch [1/5], Step [200/600], Loss: 0.5770\n",
            "Epoch [1/5], Step [300/600], Loss: 0.5421\n",
            "Epoch [1/5], Step [400/600], Loss: 0.5290\n",
            "Epoch [1/5], Step [500/600], Loss: 0.3941\n",
            "Epoch [1/5], Step [600/600], Loss: 0.3185\n",
            "Epoch [2/5], Step [100/600], Loss: 0.3736\n",
            "Epoch [2/5], Step [200/600], Loss: 0.3762\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1183\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1322\n",
            "Epoch [2/5], Step [500/600], Loss: 0.2168\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1995\n",
            "Epoch [3/5], Step [100/600], Loss: 0.2650\n",
            "Epoch [3/5], Step [200/600], Loss: 0.1380\n",
            "Epoch [3/5], Step [300/600], Loss: 0.2775\n",
            "Epoch [3/5], Step [400/600], Loss: 0.2308\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1507\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0393\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0893\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1023\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1043\n",
            "Epoch [4/5], Step [400/600], Loss: 0.2227\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0887\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0834\n",
            "Epoch [5/5], Step [100/600], Loss: 0.3288\n",
            "Epoch [5/5], Step [200/600], Loss: 0.2657\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0731\n",
            "Epoch [5/5], Step [400/600], Loss: 0.2027\n",
            "Epoch [5/5], Step [500/600], Loss: 0.2349\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1625\n",
            "Accuracy on test set: 96.69%\n"
          ]
        }
      ]
    }
  ]
}